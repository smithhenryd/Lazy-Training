\documentclass{article}

\setlength{\parskip}{1em}
\setlength{\parindent}{0pt}

% Formatting and images 
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage[titletoc,title]{appendix}
\usepackage{authblk}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{soul}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{booktabs}
\usepackage{indentfirst}
\usepackage{csquotes}

%% Packages for mathematical typesetting
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{pgf}
\usepackage{comment}
\usepackage{float}
\usepackage{blindtext}
\usepackage{enumitem}
\usepackage{bbm}

\allowdisplaybreaks

\newtheorem{manualtheoreminner}{Theorem}
\newenvironment{manualtheorem}[1]{%
  \renewcommand\themanualtheoreminner{#1}%
  \manualtheoreminner
}{\endmanualtheoreminner}

\newtheorem{manualpropositioninner}{Proposition}
\newenvironment{manualproposition}[1]{%
  \renewcommand\themanualpropositioninner{#1}%
  \manualpropositioninner
}{\endmanualtheoreminner}

\newtheorem*{assumption}{Assumption}

% Title content
\title{\textbf{A Theoretical Analysis of \enquote{Lazy Training}\\ in Deep Learning}}
\author[]{Henry Smith}
\affil[]{\normalsize Yale University}
\date{\today}

\begin{document}

\maketitle

% Abstract
\begin{abstract}
    \noindent \end{abstract}

\pagebreak

\section{Introduction}\label{introduction}

The problem of optimizing the weights of a neural network is, in general, a highly nonconvex one. Indeed, in even the simplest of models--those with a single hidden layer, for instance--we observe that the network function is highly nonconvex as a function of its parameter space at each fixed input. While the theoretical results for nonconvex optimization problems are considerably less desirable than their convex counterparts, this has not stopped practitioners from applying gradient-based methods to train neural networks (batch gradient descent, stochastic gradient descent, Adam, etc.). What actually occurs during network training, though, is a more nebulous topic. 

In particular, we will study \enquote{implicit biases} in gradient descent when training the weights of a neural network. Intuitively, an \enquote{implicit bias} means that, under certain circumstances, gradient descent behaves in a predictable way and results in a network with certain properties. The implicit bias in which we are interested has been coined \enquote{lazy training} by Chizat, Oyallon, and Bach in their 2018 paper \enquote{On Lazy Training in Differentiable Programming}. In the lazy training regime, a network behaves as a linearization around its initialization, and so training a model which is highly nonconvex in its parameters is simplified to a training an affine model. When the network is identically zero at its initialization, this means that training is equivalent to a kernel method with feature map given by the gradient of the network at its initialization. Of course, it cannot generally be true that networks are trained in the lazy regime, and so we wish to prove some formal results about when lazy training occurs.

We structure our report of lazy training as follows. In Section \ref{prelim}, we introduce mathematical notation that will be helpful for discussing and proving the theoretical results in Section \ref{theory}. This section is also of particular importance as it defines the \enquote{linearized model,} which forms the basis of lazy training. In Section \ref{theory}, we state, prove, and discuss the implications of three main results from \enquote{On Lazy Training in Differentiable Programming} by Chizat, Oyallon and Bach. These results constitute the fundamental theory of lazy training, suggesting under what conditions lazy training occurs and how it is realized throughout training. We conclude our discussion of lazy training in Section \ref{extensions} by suggesting some extensions of the results from \cite{chizat2018lazy}. In particular, we mention the properties (i.e. biases) of those models trained with lazy training and suggest some settings in which non-lazy training is preferable.

\section{Preliminaries}\label{prelim}

Having provided some intuition for lazy training, we proceed to formalize it mathematically. For the sake of convenience, the notation we use is the same as that presented in \cite{chizat2018lazy}.

We will consider $\mathbb{R}^p$ a parameter space, $\mathcal{F}$ a Hilbert space, $h: \mathbb{R}^p \rightarrow \mathcal{F}$ a model, and $R: \mathcal{F} \rightarrow \mathbb{R}_+$ a loss function. Notice here that $h$ does not map inputs to outputs, but rather a vector of parameters to an element of the Hilbert space $\mathcal{F}$.

In our particular setting of neural networks, we choose $\mathcal{F}$ to be the Hilbert space consisting of all possible network functions. As a familiar example, suppose that we are given training data $\{ (\boldsymbol{x}_i, y_i)\}_{i=1}^N$, $\boldsymbol{x}_i \in \mathbb{R}^d$, $y_i \in \mathbb{R}$ and let $\mathcal{D}$ be the corresponding empirical distribution (i.e. $\mathbb{P}( (\boldsymbol{x}, y) = (\boldsymbol{x}_1, y_1)) = \frac{1}{N} \sum_{i=1}^N \mathbbm{1}_{(\boldsymbol{x}_i, y_i) = (\boldsymbol{x}_1, y_1)}$). Further, let $\mathcal{D}_{\boldsymbol{x}}$ be the $\boldsymbol{x}$ marginal distribution of $\mathcal{D}$. Then we can choose our Hilbert space $\mathcal{F}$ to be $L^2(\mathcal{D}_{\boldsymbol{x}}, \mathbb{R}^d)$, which consists of those functions which are square integrable with respect to $\mathcal{D}_{\boldsymbol{x}}$. More generally, we can choose $\mathcal{F} = L^2(\rho_{\boldsymbol{x}}, \mathbb{R}^d)$, where $\rho_{\boldsymbol{x}}$ is any probability measure on the input space $\mathbb{R}^d$ \cite{chizat2018lazy}. In the case that $\mathcal{F}$ is a function space with $f: \mathbb{R}^d \rightarrow \mathbb{R}$ for each $f \in \mathcal{F}$, we let $h: \boldsymbol{w} \mapsto f(\boldsymbol{w}, \cdot)$ denote the map from parameter vector $\boldsymbol{w}$ to network function $f(\boldsymbol{w}, \boldsymbol{x}), \ \boldsymbol{x} \in \mathbb{R}^d$. To continue on with our previous example, we could then choose our loss function to be $R(h(\boldsymbol{w})) = \mathbb{E}_{(\boldsymbol{x}, y) \sim \mathcal{D}} \left[ (y - f(\boldsymbol{w}, \boldsymbol{x}))^2 \right]$, which is the mean-squared error, or equivalently the empirical risk corresponding to the square loss.

Throughout our paper, we will only be interested in those models $h$ which are differentiable in $\boldsymbol{w} \in \mathbb{R}^p$ as well as those loss functions $R$ which are differentiable in $f \in \mathcal{F}$. This is because we will use gradient-based methods to minimize the scaled objectives (\ref{scaledobjective}), which clearly necessitates that each of $h$ and $R$ is differentiable. We formally state our assumption on $h$ and $R$ as it is given by Chizat and colleagues:
\begin{assumption}[from \cite{chizat2018lazy}]\label{assumption1}
The model $h: \mathbb{R}^p \rightarrow \mathcal{F}$ is differentiable with a locally Lipschitz differential $Dh$. When we specify that $Dh$ is locally Lipschitz, we are referring to the map $\boldsymbol{w} \mapsto Dh(\boldsymbol{w})$, and so the Lipschitz constant is defined with respect to the operator norm. Moreover, $R: \mathcal{F} \rightarrow \mathbb{R}_+$ is differentiable with a Lipschitz gradient.
\end{assumption}

Now that we have made clear the model $h$ of interest as well as the assumptions on $h$, we introduce the linearization of $h$ around its initialization. In particular, given a model $h$ as well as some initialization $\boldsymbol{w}_0 \in \mathbb{R}^p$, we define the linearized model to be 
\begin{align}
\bar{h}(\boldsymbol{w}) = h(\boldsymbol{w}_0) + Dh(\boldsymbol{w}_0)(\boldsymbol{w} - \boldsymbol{w}_0), \quad \boldsymbol{w} \in \mathbb{R}^p\label{linearizedmodel}.
\end{align}
Once again, for the particular case of $h$ mapping a parameter vector to a neural network $h: \boldsymbol{w} \mapsto f(\boldsymbol{w}, \cdot)$, we get
\begin{align*}
    \bar{f}(\boldsymbol{w}, \boldsymbol{x}) = f(\boldsymbol{w}_0, \boldsymbol{x}) + D_{\boldsymbol{w}} f(\boldsymbol{w}_0, \boldsymbol{x})(\boldsymbol{w} - \boldsymbol{w}_0) \quad \boldsymbol{x} \in \mathbb{R}^d, \quad \boldsymbol{w} \in \mathbb{R}^p. 
\end{align*}
In even greater specificity, when the output of the network is one-dimensional $f(\boldsymbol{w}, \cdot): \mathbb{R}^d \rightarrow \mathbb{R}$, then our linearized model is
\begin{align}
    \bar{f}(\boldsymbol{w}, \boldsymbol{x}) =& f(\boldsymbol{w}_0, \boldsymbol{x}) + \nabla_w f(\boldsymbol{w}_0, \boldsymbol{x})(\boldsymbol{w} - \boldsymbol{w}_0) \nonumber\\
    =& f(\boldsymbol{w}_0, \boldsymbol{x}) + \langle \nabla_w f(\boldsymbol{w}_0, \boldsymbol{x}), \boldsymbol{w} - \boldsymbol{w}_0 \rangle \quad \boldsymbol{x} \in \mathbb{R}^d, \quad \boldsymbol{w} \in \mathbb{R}^p\label{linearizednetwork}.
\end{align}
One will discern that for this case of $f(\boldsymbol{w}, \cdot): \mathbb{R}^d \rightarrow \mathbb{R}$, $\bar{h}$ is no more than a first-order Taylor expansion of the model $h$ around its initialization $\boldsymbol{w}_0$.

So far, we have suggested two mathematical objects of interest, the model $h$ and its corresponding linearized model $\bar{h}$. For each vector $\boldsymbol{w} \in \mathbb{R}^p$ we compute the misfit of $h(\boldsymbol{w})$ and $\bar{h}(\boldsymbol{w})$ according to $R(h(\boldsymbol{w}))$ and $R(\bar{h}(\boldsymbol{w}))$, respectively. However, rather than dealing with $R(h(\boldsymbol{w}))$ and $R(\bar{h}(\boldsymbol{w}))$, Chizat and colleagues consider the objective functions corresponding to the scaled models $\alpha h$ and $\alpha \bar{h}$ for some $\alpha > 0$:
\begin{align}
    F_{\alpha}(\boldsymbol{w}) = \frac{1}{\alpha^2}R(\alpha h(\boldsymbol{w})) \qquad
    \bar{F}_{\alpha}(\boldsymbol{w}) = \frac{1}{\alpha^2}R(\alpha \bar{h}(\boldsymbol{w}))\label{scaledobjective}.
\end{align}
Here, we are doing no more than scaling the output of each of $h$ and $\bar{h}$ by a positive factor $\alpha > 0$. One should notice that the factor of  $\frac{1}{\alpha^2}$ which appears in (\ref{scaledobjective}) is simply a positive normalization factor and does not affect the minima of the objective functions (that is, $\frac{1}{\alpha^2}R(\alpha h(\boldsymbol{w}))$ and $R(\alpha h(\boldsymbol{w}))$ have the same set of minimizers).

Corresponding to the scaled objective functions $F_{\alpha}(\boldsymbol{w})$ and $\bar{F}_{\alpha}(\boldsymbol{w})$ we define the gradient flow dynamics, denoted $(\boldsymbol{w}_{\alpha}(t))_{t \geq 0}$ and $(\boldsymbol{\bar{w}}_{\alpha}(t))_{t \geq 0}$, respectively, with $\boldsymbol{w}_{\alpha}(0) = \boldsymbol{\bar{w}}_{\alpha}(0) = \boldsymbol{w}_0$. The gradient flow of $F_{\alpha}$ is a path in the parameter space space $\mathbb{R}^p$ that solves the initial value problem
\begin{align}\label{gradflow}
    \boldsymbol{w}_{\alpha}'(t) = - \nabla F_{\alpha}(\boldsymbol{w}_{\alpha}(t)), \quad \boldsymbol{w}_{\alpha}(0) = \boldsymbol{w}_0.
\end{align}
The gradient flow of $\bar{F}_{\alpha}$ is defined analogously. Of key interest to practitioners of machine learning is gradient descent, which can be thought of as a discrete time version of the gradient flow dynamics \cite{wibisono2016}. Specifically, using the forward Euler discretization of the  gradient flow dynamics with stepsize $\eta > 0$, we get $(\boldsymbol{w}_{\alpha}(t + 1) - \boldsymbol{w}_{\alpha}(t))/ \eta = - \nabla  F_{\alpha}(\boldsymbol{w}_{\alpha}(t)) \Leftrightarrow \boldsymbol{w}_{\alpha}(t + 1) = \boldsymbol{w}_{\alpha}(t) - \eta \nabla F_{\alpha}(\boldsymbol{w}_{\alpha}(t))$ for each $t \in \mathbb{N} \cup \{0\}$, which is exactly equal to the $t+1$ gradient descent update.

We mention that when the model $h$ is $m$-positive homogeneous, then scaling the model output by $\alpha$ is equivalent to scaling the model weights by $\alpha^{1/m}$. That is, $h(\alpha \boldsymbol{w}) = \alpha^m h(\boldsymbol{w})$ for every $\boldsymbol{w} \in \mathbb{R}^p$ and each $\alpha > 0$. Therefore, for $m$-positive homogeneous model $h$, the gradient flow on $\frac{1}{\alpha^2}R(\alpha h(\boldsymbol{w}))$ with $\boldsymbol{w}_{\alpha}(0) = \boldsymbol{w}_0$ is equivalent to the gradient flow on $\frac{1}{\alpha^2}R(h(\boldsymbol{w}))$ with $\boldsymbol{w}_{\alpha}(0) = \alpha^{1/m}\boldsymbol{w}_0$.

Under suitable conditions on the model $h$ and the loss function $R$, \cite{chizat2018lazy} proves that as $\alpha \rightarrow \infty$, the gradient flow of $F_{\alpha}(\boldsymbol{w})$ approaches that of $\bar{F}_{\alpha}(\boldsymbol{w})$. That is to say, for a neural network that is positive homogeneous its weights, by taking the scale with which we initialize the weights to infinity, then training the model  $h$ with gradient flow is equivalent to training the linearized model $\bar{h}$. The specific details of these results from \cite{chizat2018lazy} are the primary focus of Section \ref{theory}.

\section{Theoretical Results}\label{theory}

Now that we have rigorously defined the linearized model $\bar{h}$ as well as the gradient flow on $F_{\alpha}(\boldsymbol{w})$ and $\bar{F}_{\alpha}(\boldsymbol{w})$, we are well-equipped to study the key results from \cite{chizat2018lazy} regarding lazy training. In particular, we will characterize the relationship between the gradient flow paths $(\boldsymbol{w}_{\alpha}(t))_{t \geq 0}$ and $(\boldsymbol{\bar{w}}_{\alpha}(t))_{t \geq 0}$ as well as the corresponding predictor functions $\alpha h(\boldsymbol{w}_{\alpha}(t))$ and $\alpha \bar{h}(\boldsymbol{w}_{\alpha}(t))$ evaluated at the gradient flow as the scale of the model output $\alpha \rightarrow \infty$. By way of discussing and proving these theorems, we will gain a deeper understanding of lazy training, particularly as it pertains to neural network optimization.

\subsection{Finite-time Bounds}\label{finitebounds}

The first result that we consider relates the gradient flow dynamics of $F_{\alpha}(\boldsymbol{w})$ and those of $\bar{F}_{\alpha}(\boldsymbol{w})$ in the limit $\alpha \rightarrow \infty$ for a finite time horizon. In particular, the result we will prove from \cite{chizat2018lazy} demonstrates that at any time $t \geq 0$, the gradient flow of $F_{\alpha}(\boldsymbol{w})$ at time $t$, $\boldsymbol{w}_{\alpha}(t)$, is equivalent to that of $\bar{F}_{\alpha}(\boldsymbol{w})$ at time $t$, $\boldsymbol{\bar{w}}_{\alpha}(t)$, in the  $\alpha \rightarrow \infty$ limit. Therefore, the $t \rightarrow \infty$ limit reached by the gradient flow of $F_{\alpha}$, $\lim_{t \to \infty} \boldsymbol{w}_{\alpha}(t)$, is the same as that reached by the gradient flow of $\bar{F}_{\alpha}$, $\lim_{t \to \infty} \boldsymbol{\bar{w}}_{\alpha}(t)$, in the $\alpha \rightarrow \infty$ limit. That is to say, we observe lazy training as the scale of the model output $\alpha > 0$ grows large. This gives us our first explicit characterization of when lazy training occurs. We state the relevant theorem and proceed to prove the result:

\begin{manualtheorem}{2.2}[from \cite{chizat2018lazy}]\label{finitehorizon}
Assume that $h(\boldsymbol{w}_0) = 0$. Given a fixed time horizon $T > 0$, it holds that $\sup_{t \in [0, T]} \Vert \boldsymbol{w}_{\alpha}(t) - \boldsymbol{w}_0 \Vert = \mathcal{O}(1/\alpha)$,
\begin{align*}
    \sup_{t \in [0, T]} \Vert \boldsymbol{w}_{\alpha}(t) - \boldsymbol{\bar{w}}_{\alpha}(t) \Vert = \mathcal{O}(1/\alpha^2) \quad \text{and} \quad \sup_{t \in [0, T]} \Vert \alpha h(\boldsymbol{w}_{\alpha}(t)) - \alpha \bar{h}(\boldsymbol{\bar{w}}_{\alpha}(t)) \Vert = \mathcal{O}(1/\alpha).
\end{align*}
\end{manualtheorem}

\begin{proof} For both this proof and that of Theorem \ref{finitehorizon} in Section \ref{finitehorizon} it will be useful to define $y(t) = \alpha h(\boldsymbol{w}_{\alpha}(t))$ and $\bar{y}(t) = \alpha \bar{h}(\boldsymbol{\boldsymbol{\bar{w}}}_{\alpha}(t))$ to be the dynamics in $\mathcal{F}$. That is, $y(t)$ is simply the scaled model $\alpha h(\boldsymbol{w})$ evaluated along the gradient flow path of $F_{\alpha}(\boldsymbol{w})$, $(\boldsymbol{w}_{\alpha}(t))_{t \geq 0}$, $\boldsymbol{w}_{\alpha}(0) = \boldsymbol{w}_0$, that we previously discussed. 

To be consistent with the notation from \cite{chizat2018lazy}, we define $\Sigma(\boldsymbol{w}) := Dh(\boldsymbol{w})Dh(\boldsymbol{w})^T$ to be the neural tangent kernel (NTK) at weight vector $\boldsymbol{w} \in \mathbb{R}^p$ \cite{jacot2018neural}. The neural tangent kernel has gained recent popularity in the subject of theoretical deep learning due to the fact that in the limit $\alpha \rightarrow \infty$, the gradient flow (\ref{gradflow}) with appropriate model and loss function is no more than a kernel method with kernel given by the NTK. While we do not have sufficient space to flesh out this result, we suggest \cite{jacot2018neural} and \cite{chizat2018lazy} as references regarding the neural tangent kernel. From our definition, it is evident that $\Sigma(\boldsymbol{w})$ defines a quadratic form on $\mathcal{F}$ given by $f \mapsto f\Sigma(\boldsymbol{w})f$ for each $f \in \mathcal{F}$. Using the neural tangent kernel $\Sigma(\boldsymbol{w})$, we can say that $y(t)$ and $\bar{y}(t)$ must solve the differential equations
\begin{align*}
    \frac{d}{dt}y(t) &= \alpha \frac{d}{dt}h(\boldsymbol{w}_{\alpha}(t)) = \alpha Dh(\boldsymbol{w}(t)) \frac{d}{dt}\boldsymbol{w}_{\alpha}(t) = -\alpha Dh(\boldsymbol{w}(t)) \nabla F_{\alpha}(\boldsymbol{w}_{\alpha}(t))\\
    &=  -\alpha Dh(\boldsymbol{w}_{\alpha}(t)) \left( \alpha Dh(\boldsymbol{w}_{\alpha}(t))^T \right) \left( \frac{1}{\alpha^2} \nabla R(\alpha h(\boldsymbol{w}_{\alpha}(t))) \right) \\
    &= -\Sigma(\boldsymbol{w}_{\alpha}(t)) \nabla R(\alpha h(\boldsymbol{w}_{\alpha}(t)))\\
    &= -\Sigma(\boldsymbol{w}_{\alpha}(t)) \nabla R(y(t)).\\
    %
    \frac{d}{dt}\bar{y}(t) &= \alpha  \frac{d}{dt}\bar{h}(\boldsymbol{\bar{w}}_{\alpha}(t)) = \alpha D\bar{h}(\boldsymbol{\bar{w}}_{\alpha}(t)) \frac{d}{dt}\boldsymbol{\bar{w}}_{\alpha}(t) =  \alpha D\bar{h}(\boldsymbol{w}_{\alpha}(0))\frac{d}{dt}\boldsymbol{\bar{w}}_{\alpha}(t) = \alpha D\bar{h}(\boldsymbol{w}_{\alpha}(0)) \nabla \bar{F}_{\alpha}(\boldsymbol{\bar{w}}_{\alpha}(t))\\
    &= \alpha D\bar{h}(\boldsymbol{w}_{\alpha}(0)) \left( \alpha D\bar{h}(\boldsymbol{\bar{w}}_{\alpha}(t))^T \right)\left( \frac{1}{\alpha^2} \nabla R(\alpha \bar{h}(\boldsymbol{\bar{w}}_{\alpha}(t))) \right)\\
    &= -\Sigma(\boldsymbol{w}_{\alpha}(0))\nabla R(\boldsymbol{\bar{w}}_{\alpha}(t))\\
    &= -\Sigma(\boldsymbol{w}_{\alpha}(0))\nabla R(\bar{y}(t))
\end{align*}
with initial condition $y(0) = \bar{y}(0) = \alpha h(\boldsymbol{w}_0)$. Here, we employ the chain rule since, by our assumptions, $h: \mathbb{R}^p \rightarrow \mathcal{F}$ and $R: \mathcal{F} \rightarrow \mathbb{R}_+$ are everywhere differentiable on their domains. Besides the chain rule, the main result that we use in these two derivations is that $\boldsymbol{w}(t)$ and $\boldsymbol{\bar{w}}(t)$ evolve according to the gradient flow dynamics (\ref{gradflow}). Additionally, from the definition of the linearized model $\bar{h}$, we rewrite
\begin{align*}
  D\bar{h}(\boldsymbol{\bar{w}}_{\alpha}(t)) &= D\bigg( h(\boldsymbol{\bar{w}}_{\alpha}(0)) + Dh(\boldsymbol{\bar{w}}_{\alpha}(0))(\boldsymbol{\bar{w}}_{\alpha}(t) - \boldsymbol{\bar{w}}_{\alpha}(0))  \bigg)\\
  &= Dh(\boldsymbol{\bar{w}}_{\alpha}(0)) = Dh(\boldsymbol{w}_{\alpha}(0)).
\end{align*}
That is, $\bar{h}$ is an affine model whose derivative at all input vectors $\boldsymbol{w} \in \mathbb{R}^p$ is equal to the derivative of $h$ at its initialization $\boldsymbol{w}_{\alpha}(0) = \boldsymbol{w}_0$. Now that we have described $y(t)$ and $\bar{y}(t)$ as well as the differential equations that they must satisfy, we are prepared to proceed with our proof.

Accordingly, let $T > 0$ be an arbitrary time horizon for the gradient flow on $F_{\alpha}(\boldsymbol{w})$ and $\bar{F}_{\alpha}(\boldsymbol{w})$. We will first tackle the statement $\sup_{t \in [0, T]} \|\boldsymbol{w}_{\alpha}(t) - \boldsymbol{w}_0 \|_2 = \mathcal{O}(1/\alpha)$. This result will give us a bound on how far the gradient flow path $\boldsymbol{w}_{\alpha}(t)$ moves from its initialization $\boldsymbol{w}_{\alpha}(0)$ on the interval $[0, T]$. In fact, it will tell us that in the limit $\alpha \rightarrow \infty$, the gradient flow path on $F_{\alpha}(\boldsymbol{w})$ at any time $t \geq 0$, $\boldsymbol{w}_{\alpha}(t)$, remains fixed at the initialization $\boldsymbol{w}_{\alpha}(0)$. This provides another characterization of lazy training that we have not yet discussed: lazy training is truly \enquote{lazy} in the sense that the gradient flow path remains close to its initialization.

First, by the Fundamental Theorem of Calculus and properties of the integral it holds that for each $t \in [0, T]$,
\begin{align*}
    \|\boldsymbol{w}_{\alpha}(t) - \boldsymbol{w}_{0} \|_2 = \|\boldsymbol{w}_{\alpha}(t) - \boldsymbol{w}_{\alpha}(0) \|_2 = \left\Vert \int_0^t \boldsymbol{w}_{\alpha}'(s) \ ds  \right\Vert_2 \leq  \int_0^t \| \boldsymbol{w}_{\alpha}'(s) \|_2 \ ds \leq \int_0^T \| \boldsymbol{w}_{\alpha}'(s) \|_2 \ ds.
\end{align*}
Note that $\boldsymbol{w}_{\alpha}'(\cdot): \mathbb{R}_+ \rightarrow \mathbb{R}^p$, and so here we let
\begin{align*}
    \int_0^t \boldsymbol{w}_{\alpha}'(s) \ ds := \left( \int_0^t (\boldsymbol{w}_{\alpha})_1'(s) \ ds, \ldots, \int_0^t (\boldsymbol{w}_{\alpha})_p'(s) \ ds  \right) \in \mathbb{R}^p.
\end{align*}
Thus, in order to determine a bound on $\sup_{t \in [0, T]} \|\boldsymbol{w}_{\alpha}(t) - \boldsymbol{w}_0 \|_2$, it suffices to bound the right-hand expression. In particular, we have
\begin{align*}
    \sup_{t \in [0, T]} \|\boldsymbol{w}_{\alpha}(t) - \boldsymbol{w}_0 \|_2 &\leq \int_0^T \| \boldsymbol{w}_{\alpha}'(s) \|_2 \ ds\\
    &=\int_0^T \| \nabla F_{\alpha}(\boldsymbol{w}_{\alpha}(s)) \|_2 \ ds & \text{definition of gradient flow (\ref{gradflow})}\\
    &= \int_0^T \mathbbm{1} \cdot \| \nabla F_{\alpha}(\boldsymbol{w}_{\alpha}(s)) \|_2 \ ds \\
    &\leq \sqrt{T} \left( \int_0^T \| \nabla F_{\alpha}(\boldsymbol{w}_{\alpha}(s)) \|_2^2 \ ds \right)^{1/2}. & \text{Cauchy-Schwarz for $L^2([0, T])$}
\end{align*}
In order to invoke Cauchy-Schwarz in the final line, we must have $ \|\boldsymbol{w}_{\alpha}'(t) \|_2 \in L^2([0, T])$. This is true because $\| \boldsymbol{w}_{\alpha}'(t) \|_2 = \| \nabla F_{\alpha}(\boldsymbol{w}_{\alpha}(t)) \|_2$, where each of $\nabla F_{\alpha}(\boldsymbol{w}_{\alpha}(t)): \mathbb{R}_+ \rightarrow \mathbb{R}^p$ and $\| \cdot \|_2: \mathbb{R}^p \rightarrow \mathbb{R}_+$ is continuous. Accordingly, is true that $\|\boldsymbol{w}_{\alpha}'(t) \|_2$ is continuous on the closed interval $[0, T]$, and so it belongs to $L^2([0, T])$.

Now to simplify the integrand, we use the fact that 
\begin{align*}
\frac{d}{dt}F_{\alpha}(\boldsymbol{w}_{\alpha}(t)) = \nabla F_{\alpha}(\boldsymbol{w}_{\alpha}(t))^T \boldsymbol{w}_{\alpha}'(t) = \nabla F_{\alpha}(\boldsymbol{w}_{\alpha}(t))^T (- \nabla F_{\alpha}(\boldsymbol{w}_{\alpha}(t))) = - \| \nabla F_{\alpha}(\boldsymbol{w}_{\alpha}(t))\|_2^2.
\end{align*}
This follows from a straightforward application of the chain rule as well as the knowledge that $\boldsymbol{w}_{\alpha}(t)$ evolves according to the gradient flow dynamics on the scaled objective function $F_{\alpha}(\boldsymbol{w})$, (\ref{gradflow}).

Substituting this expression for $\| \nabla F_{\alpha}(\boldsymbol{w}_{\alpha}(t))\|_2^2$ back into the integral, we get
\begin{align*}
    \sup_{t \in [0, T]} \|\boldsymbol{w}_{\alpha}(t) - \boldsymbol{w}_0 \|_2 &\leq \sqrt{T} \left( \int_0^T \| \nabla F_{\alpha}(\boldsymbol{w}_{\alpha}(s)) \|_2^2 \ ds \right)^{1/2}\\
    &= \sqrt{T} \left( \int_0^T -\frac{d}{ds}F_{\alpha}(\boldsymbol{w}_{\alpha}(s)) \ ds \right)^{1/2}\\
    &= \sqrt{T} \left(  F_{\alpha}(\boldsymbol{w}_{\alpha}(0)) - F_{\alpha}(\boldsymbol{w}_{\alpha}(T)) \right)^{1/2}. & \text{Fundamental Theorem of Calculus}
\end{align*}
And since the loss function $R: \mathcal{F} \rightarrow \mathbb{R}_+$, then we get
\begin{align*}
    \sup_{t \in [0, T]} \|\boldsymbol{w}_{\alpha}(t) - \boldsymbol{w}_0 \|_2 &\leq \sqrt{T} \left(  F_{\alpha}(\boldsymbol{w}_{\alpha}(0)) - F_{\alpha}(\boldsymbol{w}_{\alpha}(T)) \right)^{1/2}\\
    &= \sqrt{T} \left( \frac{1}{\alpha^2} \bigg( R(\alpha h(\boldsymbol{w}_{\alpha}(0))) - R(\alpha h(\boldsymbol{w}_{\alpha}(T))) \bigg) \right)^{1/2}\\
    &\leq \sqrt{T} \left( \frac{1}{\alpha^2} \bigg( R(\alpha h(\boldsymbol{w}_{\alpha}(0))) \bigg) \right)^{1/2}\\
    &= \frac{1}{\alpha} \bigg(T \cdot R(\alpha h(\boldsymbol{w}_{\alpha}(0))) \bigg)^{1/2}.
\end{align*}

Therefore, we conclude that for each $T > 0$,
\begin{align*}
    \sup_{t \in [0, T]} \|\boldsymbol{w}_{\alpha}(t) - \boldsymbol{w}_0 \|_2 \leq \frac{1}{\alpha} \bigg(T \cdot R(\alpha h(\boldsymbol{w}_{\alpha}(0))) \bigg)^{1/2} = \mathcal{O}(1/\alpha),
\end{align*}
as we wished to prove. Notice that, although the $\mathcal{O}(1/\alpha)$ hides the dependence on $T$, our bound on $\sup_{t \in [0, T]} \|\boldsymbol{w}_{\alpha}(t) - \boldsymbol{w}_0 \|_2$ grows sublinearly in $T$. In order to achieve a bound which does not depend on this finite time horizon $T$, we will need the stronger assumptions on $h$ and $R$ that appear in Theorem \ref{finitehorizon}.

As a consequence of this bound on $ \sup_{t \in [0, T]} \|\boldsymbol{w}_{\alpha}(t) - \boldsymbol{w}_0 \|_2$, we get a couple additional results that will be useful throughout the remainder of our proof.

First, for $y(t)$ defined as in Section \ref{prelim}, we know that
\begin{align*}
    \sup_{t \in [0, T]} \|y(t) - y(0)\|_{\mathcal{F}} = \sup_{t \in [0, T]} \|\alpha h(\boldsymbol{w}_{\alpha}(t)) - \alpha h(\boldsymbol{w}_{\alpha}(0)) \|_{\mathcal{F}} = \sup_{t \in [0, T]} \alpha \| h(\boldsymbol{w}_{\alpha}(t))\|_{\mathcal{F}}.
\end{align*}
But from the result we just proved, we also know that for every $t \in [0, T]$, $\boldsymbol{w}_{\alpha}(t) \in B_{\epsilon}(\boldsymbol{w}_0)$, where $\epsilon = \frac{C}{\alpha}$ for some constant $C \geq 0$. Here $B_{\epsilon}(\boldsymbol{w}_0)$ denotes the closed Euclidean ball of radius $\epsilon$ centered at $\boldsymbol{w}_0$. And since $h: \mathbb{R}^p \rightarrow \mathcal{F}$ is continuous by assumption, as is $\| \cdot \|_{\mathcal{F}}: \mathcal{F} \rightarrow \mathbb{R}_+$, then the composition $\boldsymbol{w} \mapsto \| h(\boldsymbol{w}) \|_{\mathcal{F}}$ is continuous on $\mathbb{R}^p$. Altogether, since $\boldsymbol{w} \mapsto \| h(\boldsymbol{w}) \|_{\mathcal{F}}$ is continuous on the compact set $B_{\epsilon}(\boldsymbol{w}_0)$, then by the Weierstrauss Extreme Value Theorem, $\|h(\boldsymbol{w})\|_{\mathcal{F}} \leq C$ for every $\boldsymbol{w} \in B_{\epsilon}(\boldsymbol{w}_0)$, for some fixed $C \geq 0$. In particular, this implies that $\| h(\boldsymbol{w}_{\alpha}(t)) \|_{\mathcal{F}} \leq C$ for every $t \in [0, T]$. Thus, we conclude
\begin{align*}
    \sup_{t \in [0, T]} \|y(t) - y(0)\|_{\mathcal{F}} = \sup_{t \in [0, T]} \alpha \| h(\boldsymbol{w}_{\alpha}(t))\|_{\mathcal{F}} \leq  C \alpha.
\end{align*}

By a similar argument, we can show that $\sup_{t \in [0, T]} \|\boldsymbol{w}_{\alpha}(t) - \boldsymbol{w}_0 \|_2 = \mathcal{O}(1/ \alpha)$ implies $\sup_{t \in [0, T]} \| \nabla R(y(t))\|_{\mathcal{F}}\leq C$ for some constant $C \geq 0$. Specifically, we have assumed that that the loss function $R$ has a Lipschitz gradient, meaning that the map $f \mapsto \nabla R(f), \ f \in \mathcal{F}$ is Lipschitz. And $f \mapsto \nabla R(f)$ Lipschitz implies $f \mapsto \nabla R(f)$ continuous. Also, we know that each of $h: \mathbb{R}^p \rightarrow \mathcal{F}$ and $\| \cdot \|_{\mathcal{F}}: \mathcal{F} \rightarrow \mathbb{R}_+$ is continuous, and so altogether the composition $\|\nabla R(\alpha h(\boldsymbol{w}))\|_{\mathcal{F}}: \mathbb{R}^p \rightarrow \mathbb{R}_+$ is continuous. Therefore, we can apply the same  Weierstrauss Extreme Value result as in the previous paragraph to say that for every $\boldsymbol{w} \in B_{\epsilon}(\boldsymbol{w}_0)$ it holds that $\|\nabla R(\alpha h(\boldsymbol{w}))\|_{\mathcal{F}} \leq C$ for some fixed constant $C \geq 0$. $\sup_{t \in [0, T]} \|\boldsymbol{w}_{\alpha}(t) - \boldsymbol{w}_0 \|_2 = \mathcal{O}(1/\alpha)$ gives us that $\boldsymbol{w}_{\alpha}(t)$ is in the closed ball $B_{\epsilon}(\boldsymbol{w}_0)$ for every $t \in [0, T]$ with appropriate choice of $\epsilon \geq 0$. Therefore, we conclude
\begin{align*}
    \sup_{t \in [0, T]} \| \nabla R(y(t))\|_{\mathcal{F}} = \sup_{t \in [0, T]} \| \nabla R(\alpha h(\boldsymbol{w}_{\alpha}(t)))\|_{\mathcal{F}} \leq C.
\end{align*}

We continue on by proving the bound $\sup_{t \in [0, T]} \Vert \alpha h(\boldsymbol{w}_{\alpha}(t)) - \alpha \bar{h}(\boldsymbol{\bar{w}}_{\alpha}(t)) \Vert_{\mathcal{F}} = \mathcal{O}(1/\alpha).$ While the first result established a bound on the distance of between the gradient flow path and its initialization on the interval $[0, T]$, this result will bound the distance between the scaled model $\alpha h$ and its linearized counterpart $\alpha \bar{h}$ evaluated along their respective gradient flow paths $(\boldsymbol{w}_{\alpha}(t))_{t \geq 0}$ and $(\boldsymbol{\bar{w}}_{\alpha}(t))_{t \geq 0}$ on $[0, T]$. Consequently, we observe that as $\alpha \rightarrow \infty$ the scaled original model $\alpha h$ evaluated at $\boldsymbol{w}_{\alpha}(t)$ is equivalent to the scaled linearized model $\alpha \bar{h}$ evaluated at $\boldsymbol{\bar{w}}_{\alpha}(t)$ for any time $t \geq 0$.

To start off our proof, we recall our notation $y(t) = \alpha h(\boldsymbol{w}_{\alpha}(t))$, $\bar{y}(t) = \alpha \bar{h}(\boldsymbol{\bar{w}}_{\alpha}(t))$ from Section \ref{prelim}. With these functions $y$, $\bar{y}$, we define $\Delta(t) = \| y(t) - \bar{y}(t) \|_{\mathcal{F}}, \ \forall t \geq 0$, which is the distance between $y(t)$ and $\bar{y}(t)$ in the Hilbert space $\mathcal{F}$. By the definition of the linearized model $\bar{h}$, we know that $\Delta$ satisfies $\Delta(0) = \| y(0) - \bar{y}(0) \|_{\mathcal{F}} = |\alpha|  \| h(\boldsymbol{w}_0) - \bar{h}(\boldsymbol{w}_0) \|_{\mathcal{F}} = |\alpha| \| h(\boldsymbol{w}_0) - h(\boldsymbol{w}_0) \|_{\mathcal{F}} = 0$. Furthermore, for each $t \geq 0$ we derive an upper bound on the derivative $\Delta'(t)$: 
\begin{align*}
    \Delta'(t) &= \frac{d}{dt}\| y(t) - \bar{y}(t) \|_{\mathcal{F}}\\
    &= \frac{d}{dt}\left\Vert \left( \int_0^t \frac{d}{ds} (y(s) - \bar{y}(s)) \ ds \right) \right\Vert_{\mathcal{F}} & \text{Fundamental Theorem of Calculus}\\
    &= \frac{d}{dt}\left\Vert \left( \int_0^t y'(s) - \bar{y}'(s) \ ds \right) \right\Vert_{\mathcal{F}}\\
    &\leq \frac{d}{dt}\left( \int_0^t \left\Vert y'(s) - \bar{y}'(s) \right\Vert_{\mathcal{F}} \ ds \right)\\
    &=  \| y'(t) - \bar{y}'(t) \|_{\mathcal{F}}. & \text{Fundamental Theorem of Calculus}
\end{align*}
This was quite a slick trick and only required our differentiability assumptions on $h: \mathbb{R}^p \rightarrow \mathcal{F}$ and $R: \mathcal{F} \rightarrow \mathbb{R}_+$. Continuing on, recall the expressions for $y'(t)$ and $\bar{y}'(t)$, $t \geq 0$ that we derived in Section \ref{prelim}. Substituting them into the bound on $\frac{d}{dt} \Delta(t)$ we derived, we get
\begin{align*}
    \frac{d}{dt} \Delta(t) &\leq \| y'(t) - \bar{y}'(t) \|_{\mathcal{F}}\\
    &= \| \Sigma(\boldsymbol{w}_{\alpha}(t)) \nabla R(y(t)) -  \Sigma(\boldsymbol{w}_{\alpha}(0)) \nabla R(\bar{y}(t)) \|_{\mathcal{F}}\\
    &\leq \| \Sigma(\boldsymbol{w}_{\alpha}(t)) \nabla R(y(t)) - \Sigma(\boldsymbol{w}_{\alpha}(0)) \nabla R(y(t)) \|_{\mathcal{F}} + \| \Sigma(\boldsymbol{w}_{\alpha}(0)) \nabla R(y(t)) - \Sigma(\boldsymbol{w}_{\alpha}(0)) \nabla R(\bar{y}(t))\|_{\mathcal{F}}\\
    &= \| (\Sigma(\boldsymbol{w}_{\alpha}(t)) - \Sigma(\boldsymbol{w}_{\alpha}(0))) \nabla R(y(t))\|_{\mathcal{F}} + \| \Sigma(\boldsymbol{w}_{\alpha}(0))(\nabla R(y(t)) - \nabla R(\bar{y}(t)))\|_{\mathcal{F}}.
\end{align*}
The second inequality is achieved by adding and subtracting a term of $\Sigma(\boldsymbol{w}_{\alpha}(0)) \nabla R(y(t))$ and subsequently applying the triangle inequality for the norm $\mathcal{F}$. Next, we will invoke the properties of the operator norm, where our operator $f \mapsto \Sigma(\boldsymbol{w}) f$ maps from normed vector space $\mathcal{F}$ to itself. In particular, since $Dh(\boldsymbol{w}): \mathbb{R}^p \rightarrow \mathcal{F}$ is continuous and linear (for each $\boldsymbol{w} \in \mathbb{R}^p$), then so is $f \mapsto \Sigma(\boldsymbol{w}) f$, where $\Sigma(\boldsymbol{w}) = Dh(\boldsymbol{w})Dh(\boldsymbol{w})^T$. As a result, we have get that for each $f \in \mathcal{F}$, $\| \Sigma(\boldsymbol{w}) f\| \leq \| \Sigma(\boldsymbol{w}) \| \| f \|_{\mathcal{F}}$, where $\| \cdot \|$ denotes the operator norm. Applying this inequality to the above expression, we get
\begin{align*}
    \frac{d}{dt} \Delta(t) &\leq \|\Sigma(\boldsymbol{w}_{\alpha}(t)) - \Sigma(\boldsymbol{w}_{\alpha}(0))\| \| \nabla R(y(t))\|_{\mathcal{F}} + \| \Sigma(\boldsymbol{w}_{\alpha}(0)) \| \| \nabla R(y(t)) - \nabla R(\bar{y}(t))\|_{\mathcal{F}}.
\end{align*}

From here, we bound each of the two terms separately, starting with the first term. To consider the factor $\|\Sigma(\boldsymbol{w}_{\alpha}(t)) - \Sigma(\boldsymbol{w}_{\alpha}(0))\|$, we cite the result from \cite{chizat2018lazy} which states that $\text{Lip}(\Sigma) \leq 2 \text{Lip}(h) \text{Lip}(Dh)$. Note that $\text{Lip}(\Sigma)$ is defined with respect to the operator norm. From the first result we proved, we know that we are dealing with $\boldsymbol{w}_{\alpha}(t)$ contained in a closed Euclidean ball $B_{\epsilon}(\boldsymbol{w}_0)$ with some radius $\epsilon \geq 0$. And so $Dh$ locally Lipschitz (our assumption) on compact set $B_{\epsilon}(\boldsymbol{w}_0)$ implies $Dh$ Lipschitz on $B_{\epsilon}(\boldsymbol{w}_0)$. Also, $Dh$ continuous on $B_{\epsilon}(\boldsymbol{w}_0)$ implies that $h$ is Lipschitz on $B_{\epsilon}(\boldsymbol{w}_0)$. Letting $\text{Lip}(Dh)$ and $\text{Lip}(h)$ be the Lipschitz constants of $Dh$ and $h$ on $B_{\epsilon}(\boldsymbol{w}_0)$, respectively, we get the desired bound on $\text{Lip}(\Sigma)$. Invoking the first result that we proved, we get $\|\Sigma(\boldsymbol{w}_{\alpha}(t)) - \Sigma(\boldsymbol{w}_{\alpha}(0))\| \leq 2 \cdot \text{Lip}(h) \cdot \text{Lip}(Dh) \|\boldsymbol{w}_{\alpha}(t) - \boldsymbol{w}_{\alpha}(0) \|_2 \leq 2 \cdot \text{Lip}(h) \cdot \text{Lip}(Dh) \cdot C /\alpha$ for some constant $C \geq 0$. As for the factor $\| \nabla R(y(t))\|_{\mathcal{F}}$, we recall the result we previously proved that $\sup_{\tilde{t} \in [0, T]} \| \nabla R(y(\tilde{t}))\|_{\mathcal{F}} \leq \tilde{C}$ for some constant $\tilde{C} \geq 0$. And so, in all, we have proven that $\|\Sigma(\boldsymbol{w}_{\alpha}(t)) - \Sigma(\boldsymbol{w}_{\alpha}(0))\| \| \nabla R(y(t))\|_{\mathcal{F}} \leq C_1/\alpha$ for some constant $C_1 \geq 0$.

As for the second term, we call upon our assumption that the loss function $R$ has a Lipschitz gradient to say that $\| \nabla R(y(t)) - \nabla R(\bar{y}(t))\|_{\mathcal{F}} \leq \text{Lip}(\nabla R) \| y(t) -  \bar{y}(t) \|_{\mathcal{F}} = \text{Lip}(\nabla R)\Delta(t)$ where $\text{Lip}(\nabla R) \geq 0$ denotes the Lipschitz constant of $f \mapsto \nabla R(f)$. Accordingly, we have $\| \Sigma(\boldsymbol{w}_{\alpha}(0)) \| \| \nabla R(y(t)) - \nabla R(\bar{y}(t))\|_{\mathcal{F}} \leq C_2 \Delta(t)$ for some constant $C_2 \geq 0$.

Altogether, we have shown that
\begin{align*}
    \Delta'(t) \leq C_1/\alpha + C_2\Delta(t), \quad \Delta(0) = 0
\end{align*}
for suitable constants $C_1, C_2 \geq 0$. We notice, though, that the equation $u'(t) = C_1/\alpha + C_2 u(t)$ with initial condition $u(0) = 0$ defines a first-order, linear differential equation. This equation has a unique solution on all of $\mathbb{R}$, which we determine using an integrating factor:
\begin{align*}
    &u'(t) = C_1/\alpha + C_2 u(t)\\
    &u'(t) - C_2u(t) = C_1/\alpha\\
    & \exp(-C_2t)u(t) = \int^t C_1/\alpha  \exp(-C_2s) \ ds + C\\
    & u(t) = -C_1/(C_2\alpha) + C\exp(C_2t)\\
    & u(t) = \frac{C_1}{C_2\alpha}(\exp(C_2t) - 1). & u(0) = 0
\end{align*}
But since $\Delta'(t) \leq u'(t)$ for all times $t \geq 0$ and $\Delta(0) = u(0) = 0$, then it holds that $\Delta(t) \leq u(t)$ for all times $t \geq 0$. That is, the curve $\Delta(t)$ lies strictly below the solution curve $u(t)$ to our differential equation $u'(t) = C_1/\alpha + C_2 u(t)$, $u(0) = 0$.

And so we have proven
\begin{align*}
    \| \alpha h(\boldsymbol{w}_{\alpha}(t)) - \alpha \bar{h}(\boldsymbol{\bar{w}}_{\alpha}(t)) \|_{\mathcal{F}} =  \Delta(t) \leq \frac{C_1}{C_2\alpha}(\exp(C_2t) - 1) \leq \frac{C_1}{C_2\alpha}(\exp(C_2T) - 1). \quad \forall t \in [0, T]
\end{align*}
Therefore, we conclude
\begin{align*}
   \sup_{t \in [0, T]} \| \alpha h(\boldsymbol{w}_{\alpha}(t)) - \alpha \bar{h}(\boldsymbol{\bar{w}}_{\alpha}(t)) \|_{\mathcal{F}} \leq \frac{1}{\alpha} \left( \frac{C_1}{C_2}(\exp(C_2T) - 1) \right) = \mathcal{O}(1/\alpha).
\end{align*}
One will observe that the resulting bound is worse than than that we derived on $\sup_{t \in [0, T]} \| \boldsymbol{w}_{\alpha}(t) - \boldsymbol{w}_0 \|_2$ as there is an exponential dependence on the finite time horizon $T$. That is, fixing some initialization scale $\alpha > 0$, our upper bound on $\sup_{t \in [0, T]} \| \boldsymbol{w}_{\alpha}(t) - \boldsymbol{w}_0 \|_2$ grows exponentially as a function of $T$.

The final bound we would like to prove is that on the distance between the gradient flow paths of $F_{\alpha}(\boldsymbol{w})$ and $\bar{F}_{\alpha}(\boldsymbol{w})$: $\sup_{t \in [0, T]} \| \boldsymbol{w}_{\alpha}(t) -  \boldsymbol{\bar{w}}_{\alpha}(t) \| = \mathcal{O}(1/\alpha^2)$. This bound tells us that in the limit $\alpha \rightarrow \infty$, the gradient flow of $F_{\alpha}(\boldsymbol{w})$ is equivalent to that of $\bar{F}_{\alpha}(\boldsymbol{w})$ at any time $t \geq 0$, where $\boldsymbol{w}_{\alpha}(0) = \boldsymbol{\bar{w}}_{\alpha}(0) = \boldsymbol{w}_0$. 

Analogous to the function $\Delta(t): \mathbb{R}_+ \rightarrow 
\mathbb{R}_+$ in the previous portion of our proof, we start by defining $\delta(t) := \| \boldsymbol{w}_{\alpha}(t) - \boldsymbol{\bar{w}}_{\alpha}(t) \|_2$ for each $t \geq 0$. We approach the problem of bounding $\delta(t)$ on the interval $[0, T]$ by deriving a bound on $\delta'(t)$. 

Our first step in finding a bound on $\delta'(t)$ is very similar to that used in the previous portion of our proof. In particular, we know that $\boldsymbol{w}_{\alpha}(0) = \boldsymbol{\bar{w}}_{\alpha}(0) = \boldsymbol{w}_0$, and so $\delta(0) = 0$. Moreover, from our computations of $y'(t)$ and $\bar{y}'(t)$ in Section \ref{prelim}, we know that $\boldsymbol{w}'_{\alpha}(t) = - \nabla F_{\alpha}(\boldsymbol{w}_{\alpha}(t)) = \frac{1}{\alpha} Dh(\boldsymbol{w}_{\alpha}(t))^T \nabla R(y(t))$ as well as $\boldsymbol{\bar{w}}'_{\alpha}(t) = - \nabla \bar{F}_{\alpha}(\boldsymbol{\bar{w}}_{\alpha}(t)) = \frac{1}{\alpha} Dh(\boldsymbol{w}_{\alpha}(0))^T \nabla R(\bar{y}(t))$ for every $t \geq 0$. As a result, we get the following bound on $\delta'(t)$ for each $t \geq 0$:
\begin{align*}
    \delta'(t) =& \frac{d}{dt} \| \boldsymbol{w}_{\alpha}(t) - \boldsymbol{\bar{w}}_{\alpha}(t) \|_2\\
    =& \frac{d}{dt} \left\Vert \int_0^t \boldsymbol{w}_{\alpha}'(s) - \boldsymbol{\bar{w}}_{\alpha}'(s) \ ds \right\Vert_2  & \text{Fundamental Theorem of Calculus}\\
    \leq& \frac{d}{dt} \int_0^t \| \boldsymbol{w}_{\alpha}'(s) - \boldsymbol{\bar{w}}_{\alpha}'(s) \|_2 \ ds\\
    =& \| \boldsymbol{w}_{\alpha}'(t) - \boldsymbol{\bar{w}}_{\alpha}'(t) \|_2. & \text{Fundamental Theorem of Calculus}
\end{align*}
Substituting in our particular expressions for $\boldsymbol{w}_{\alpha}'(t)$ and $\boldsymbol{\bar{w}}_{\alpha}'(t)$, 
\begin{align*}
    \delta'(t) &\leq \| \boldsymbol{w}_{\alpha}'(t) - \boldsymbol{\bar{w}}_{\alpha}'(t) \|_2 = \frac{1}{\alpha}\|Dh(\boldsymbol{w}_{\alpha}(t))^T \nabla R(y(t)) - Dh(\boldsymbol{w}_{\alpha}(0))^T \nabla R(\bar{y}(t)) \|_2\\
    &\leq \frac{1}{\alpha}\|Dh(\boldsymbol{w}_{\alpha}(t))^T \nabla R(y(t)) -  Dh(\boldsymbol{w}_{\alpha}(0))^T \nabla R(y(t)) \|_2\\
    &+ \|Dh(\boldsymbol{w}_{\alpha}(0))^T \nabla R(y(t)) - Dh(\boldsymbol{w}_{\alpha}(0))^T \nabla R(\bar{y}(t)) \|_2\\
    &= \frac{1}{\alpha} \bigg( \|(Dh(\boldsymbol{w}_{\alpha}(t))^T -  Dh(\boldsymbol{w}_{\alpha}(0))^T)\nabla R(y(t)) \|_2 + \|Dh(\boldsymbol{w}_{\alpha}(0))^T( \nabla R(y(t)) - \nabla R(\bar{y}(t)) \|_2 \bigg)
\end{align*}
The inequality on the second line follows from adding and subtracting a term of $ Dh(\boldsymbol{w}_{\alpha}(0))^T \nabla R(y(t))$ and then invoking the triangle inequality for the $\ell^2$ norm. In order to further bound $\delta'(t)$, we use the fact that $Dh(\boldsymbol{w}): \mathbb{R}^p \rightarrow \mathcal{F}$ is a continuous, linear operator for each $\boldsymbol{w} \in \mathbb{R}^p$, and thus so is its adjoint $Dh(\boldsymbol{w})^T$, where both $\mathbb{R}^p$ and $\mathcal{F}$ are normed vector spaces. Consequently, we have that for each $f \in \mathcal{F}$, $\|Dh(\boldsymbol{w})^T f\|_2  \leq \| Dh(\boldsymbol{w})^T \| \| f \|_{\mathcal{F}}$, where $\| Dh(\boldsymbol{w})^T \|$ denotes the operator norm of $Dh(\boldsymbol{w})^T$. Also, we will use the fact that for $Dh$ a continuous, linear operator, then $\| Dh(\boldsymbol{w}) \| = \| Dh(\boldsymbol{w})^T \|$. Putting together all of these pieces, we have
\begin{align*}
    \delta'(t) &\leq \frac{1}{\alpha} \bigg(\|Dh(\boldsymbol{w}_{\alpha}(t))^T -  Dh(\boldsymbol{w}_{\alpha}(0))^T\| \|\nabla R(y(t)) \|_{\mathcal{F}} + \|Dh(\boldsymbol{w}_{\alpha}(0))^T\| \| \nabla R(y(t)) - \nabla R(\bar{y}(t)) \|_{\mathcal{F}}  \bigg)\\
    &=\frac{1}{\alpha} \bigg(\|Dh(\boldsymbol{w}_{\alpha}(t)) -  Dh(\boldsymbol{w}_{\alpha}(0))\| \|\nabla R(y(t)) \|_{\mathcal{F}} + \|Dh(\boldsymbol{w}_{\alpha}(0))\| \| \nabla R(y(t)) - \nabla R(\bar{y}(t)) \|_{\mathcal{F}}  \bigg).
\end{align*}
From here, we bound each of the two terms separately. Starting with $\|Dh(\boldsymbol{w}_{\alpha}(t)) -  Dh(\boldsymbol{w}_{\alpha}(0)) 
\|\nabla R(y(t))\|_{\mathcal{F}}$, we recall our assumption that the map $\boldsymbol{w} \mapsto Dh(\boldsymbol{w})$ is locally Lipschitz. And from the first result we also know that $\boldsymbol{w}_{\alpha}(t)$ is contained in some closed Euclidean ball centered at $\boldsymbol{w}_{\alpha}(0)$ (that is, $\boldsymbol{w}_{\alpha}(t) \in B_{\epsilon}(\boldsymbol{w}_0), \ \forall t \in [0, T]$ for appropriate choice of $\epsilon \geq 0$). Therefore, we have that $\boldsymbol{w} \mapsto Dh(\boldsymbol{w})$ is Lipschitz on the compact set $B_{\epsilon}(\boldsymbol{w}_0)$, and so $\|Dh(\boldsymbol{w}_{\alpha}(t)) -  Dh(\boldsymbol{w}_{\alpha}(0)) \| \|\nabla R(y(t))\|_{\mathcal{F}} \leq \text{Lip}(Dh) \| \boldsymbol{w}_{\alpha}(t) - \boldsymbol{w}_{\alpha}(0) \|_2\|\nabla R(y(t))\|_{\mathcal{F}}$, where $\text{Lip}(Dh)$ denotes the Lipschitz constant of $Dh$ on $B_{\epsilon}(\boldsymbol{w}_0)$. Also from the first result, we know that $\sup_{\tilde{t} \in [0, T]} \| \boldsymbol{w}_{\alpha}(\tilde{t}) - \boldsymbol{w}_{\alpha}(0) \| \leq C_1/\alpha$ for some constant $C_1 \in \mathbb{R}_+$. Similarly, we previously showed that $\sup_{\tilde{t} \in [0, T]} \| \nabla R(y(\tilde{t})) \|_{\mathcal{F}}
\leq C_2$ for some $C_2 \in \mathbb{R}_+$. Altogether, we have $\|Dh(\boldsymbol{w}_{\alpha}(t)) -  Dh(\boldsymbol{w}_{\alpha}(0)) \| \|\nabla R(y(t))\|_{\mathcal{F}} \leq C_1 \cdot C_2 \cdot \text{Lip}(Dh)/ \alpha$.

And for the second term $\|Dh(\boldsymbol{w}_{\alpha}(0))\| \| \nabla R(y(t)) - \nabla R(\bar{y}(t)) \|_{\mathcal{F}}$, we recall our assumption that the gradient of $R$, $f \mapsto \nabla R(f)$, is Lipschitz. Therefore, we have $\|Dh(\boldsymbol{w}_{\alpha}(0))\| \| \nabla R(y(t)) - \nabla R(\bar{y}(t)) \|_{\mathcal{F}} \leq \text{Lip}(\nabla R) \cdot\\ \|Dh(\boldsymbol{w}_{\alpha}(0))\|  \| y(t) - \bar{y}(t) \| \leq \text{Lip}(\nabla R)\cdot C_1  \| y(t) - \bar{y}(t) \|$ for some constant $C_1 \geq 0$. Additionally, from the second result we proved, we know that $\sup_{\tilde{t} \in [0, T]} \| \alpha h(\boldsymbol{w}_{\alpha}(\tilde{t})) - \alpha \bar{h}(\boldsymbol{\bar{w}}_{\alpha}(\tilde{t})) \| = \sup_{\tilde{t} \in [0, T]} \| y(\tilde{t}) - \bar{y}(\tilde{t}) \| \leq C_2/ \alpha$ for some constant $C_2 \geq 0$. And so we have shown $\|Dh(\boldsymbol{w}_{\alpha}(0))\| \| \nabla R(y(t)) - \nabla R(\bar{y}(t)) \|_{\mathcal{F}} \leq C_1 \cdot C_2 \cdot \text{Lip}(\nabla R)/\alpha$.

Combining these two bounds, we have
\begin{align*}
\delta'(t) &\leq \frac{1}{\alpha} \bigg(\|Dh(\boldsymbol{w}_{\alpha}(t)) -  Dh(\boldsymbol{w}_{\alpha}(0))\| \|\nabla R(y(t)) \|_{\mathcal{F}} + \|Dh(\boldsymbol{w}_{\alpha}(0))\| \| \nabla R(y(t)) - \nabla R(\bar{y}(t)) \|_{\mathcal{F}}  \bigg)\\
&\leq C/\alpha^2 
\end{align*}
for some constant $C \geq 0$, for each $t \geq 0$. Thus, we conclude that $\sup_{t \in [0, T]} \delta'(t) \leq C/\alpha^2$. And by our previous justification that $\delta(0) = 0$, then it holds that for each $t \in [0, T]$,
\begin{align*}
    \delta(t) &= \int_0^t \delta'(s) \ ds & \text{Fundamental Theorem of Calculus}\\
    &\leq \int_0^t \sup_{\tilde{t} \in [0, T]} \delta'(\tilde{t}) \ ds & \delta'(s) \leq \sup_{\tilde{t} \in [0, T]} \delta'(\tilde{t}), \ \forall s \in [0, T] \\
    &= t \sup_{\tilde{t} \in [0, T]} \delta'(\tilde{t})\\
    &\leq T \sup_{\tilde{t} \in [0, T]} \delta'(\tilde{t}) \leq T \cdot C/\alpha^2.
\end{align*}
This gives us our desired result
\begin{align*}
   \sup_{t \in [0, T]} \| \boldsymbol{w}_{\alpha}(t) -  \boldsymbol{\bar{w}}_{\alpha}(t) \|_2 = \sup_{t \in [0, T]} \delta(t) \leq T \cdot C/\alpha^2 = \mathcal{O}(1/\alpha^2).
\end{align*}

We have demonstrated each of the three bounds stated in Theorem \ref{finitehorizon}, and so we conclude our proof.
\end{proof}

So far, we have given a general characterization of lazy training that, while beneficial in the theoretical sense, is of little practical use. To summarize our results from Theorem \ref{finitehorizon}, we showed that at any time $t \geq 0$, the gradient flow path of $F_{\alpha}(\boldsymbol{w})$ at time $t$ is equivalent to that of $\bar{F}_{\alpha}(\boldsymbol{w})$ at time $t$ as $\alpha \rightarrow \infty$. Likewise, we demonstrated that for each $t \geq 0$, the original scaled model $\alpha h$, which is not a priori convex in $\boldsymbol{w} \in \mathbb{R}^p$, evaluated at $\boldsymbol{w}_{\alpha}(t)$ is equivalent to the linearized scaled model $\alpha \bar{h}$ evaluated at $\boldsymbol{\bar{w}}_{\alpha}(t)$ as $\alpha \rightarrow \infty$. Ultimately, these statements tell us that in the $\alpha \rightarrow \infty$ limit, the limit reached by gradient flow on $F_{\alpha}$ and $\bar{F}_{\alpha}$ are equivalent, $\lim_{t \to \infty} \boldsymbol{w}_{\alpha}(t) = \lim_{t \to \infty} \boldsymbol{\bar{w}}_{\alpha}(t)$, as are the models $\alpha h$ and $\alpha \bar{h}$ evaluated at $\lim_{t \to \infty} \boldsymbol{w}_{\alpha}(t) = \lim_{t \to \infty} \boldsymbol{\bar{w}}_{\alpha}(t)$. And so we have shown that lazy training as we presented it in Section \ref{introduction} occurs as the factor by which we are scaling the model output grows to infinity. If the model $h$ is positive homogeneous, we have equivalently shown that lazy training occurs when the scale of the initialization $\boldsymbol{w}_{\alpha}(0) = \alpha \boldsymbol{w}_0$ grows to infinity.

Why this result is poorly suited for practical applications, though, is due to the dependence of our bounds on the time horizon $T$. Expressly, for large times $t$ we would need a very large initialization scale $\alpha > 0$ to see the convergence of $\boldsymbol{w}_{\alpha}(t)$ to $\boldsymbol{\bar{w}}_{\alpha}(t)$ and $\alpha h(\boldsymbol{w}_{\alpha}(t))$ to $\alpha \bar{h}(\boldsymbol{\bar{w}}_{\alpha}(t))$ in the respective norms within some small threshold $\epsilon > 0$. And since to approximate the limit reached by gradient flow one must consider large $t$, this makes comparing the gradient flow limits of $F_{\alpha}(\boldsymbol{w})$ and $\bar{F}_{\alpha}(\boldsymbol{w})$ onerous. We address this problem with Theorem \ref{finitehorizon}, which, by making stronger regularity assumptions on the model $h$ and loss $R$, extends the bounds we proved in Theorem \ref{finitehorizon} to be uniform in time $t \geq 0$.


\subsubsection{Model Generalization}
Before continuing on to the uniform time case, we return to the example described in Section \ref{prelim} where the model $h$ maps each weight vector $\boldsymbol{w} \in \mathbb{R}^p$ to a network function $f(\boldsymbol{w}, \cdot) \in \mathcal{F}$. Here, we suppose that $f(\boldsymbol{w}, \cdot): \mathbb{R}^d \rightarrow \mathbb{R}^k$; notice that this is different from our discussion in Section \ref{prelim} where the output of $f(\boldsymbol{w}, \cdot)$ was one-dimensional. Also, suppose that we are given some training data $\{(\boldsymbol{x}_i, \boldsymbol{y}_i) \}_{i=1}^N$, where each $\boldsymbol{x}_i \in \mathbb{R}^d$, $\boldsymbol{y}_i \in \mathbb{R}^k$. Then by Theorem \ref{finitehorizon}, we would expect that for each $t \geq 0$ in the limit as $\alpha \rightarrow \infty$, $\| \alpha f(\boldsymbol{w}_{\alpha}(t), \boldsymbol{x}_i) -   \alpha  \bar{f}(\boldsymbol{\bar{w}}_{\alpha}(t), \boldsymbol{x}_i) \|_2$ is small for $i = 1, \ldots, N$. That is, the scaled original model evaluated at the training input points $\boldsymbol{x}_i$ along its gradient flow path should be equal to the scaled linearized model evaluated at the same points along its gradient flow path. However, it is unclear whether or not the scaled model $\alpha f(\boldsymbol{w}_{\alpha}(t), \cdot)$ generalizes like the scaled linearized model $\alpha \bar{f}(\boldsymbol{\bar{w}}_{\alpha}(t), \cdot)$. That is, we would like to know whether $\| \alpha f(\boldsymbol{w}_{\alpha}(t), \boldsymbol{x}') - \alpha  \bar{f}(\boldsymbol{\bar{w}}_{\alpha}(t), \boldsymbol{x}') \|_2$ is small for $\boldsymbol{x}' \notin \{ \boldsymbol{x}_i \}_{i=1}^N$.

In Proposition \ref{generalization}, Chizat and colleagues address this question and show that on a certain subset of the input space $\mathcal{X} \subset \mathbb{R}^d$, $\alpha f(\boldsymbol{w}_{\alpha}(t), \cdot)$ indeed generalizes like the linearized model $\alpha \bar{f}(\boldsymbol{\bar{w}}_{\alpha}(t), \cdot)$. We state the authors' proposition and then proceed to prove their result.

\begin{manualproposition}{A.1}\label{generalization}
Assume that the results of Theorem \ref{finitehorizon} hold. In particular, for some constants $C_1, C_2 > 0$ it holds that $\| \boldsymbol{w}_{\alpha}(T) - \boldsymbol{\bar{w}}_{\alpha}(T) \|_2 \leq C_1\log(\alpha)/\alpha^2$ as well $\| \boldsymbol{w}_{\alpha}(T) - \boldsymbol{w}_0 \|_2 \leq C_2\log(\alpha)/\alpha$. Assume moreover that there exists a set $\mathcal{X} \subset \mathbb{R}^d$ such that $M_1 := \sup_{\boldsymbol{x} \in \mathcal{X}} \| D_{\boldsymbol{w}}f(\boldsymbol{w}_0, \boldsymbol{x}) \| < \infty$ and $M_2 := \sup_{\boldsymbol{x} \in \mathcal{X}} \text{Lip}( \boldsymbol{w} \mapsto D_{\boldsymbol{w}}f(\boldsymbol{w}, \boldsymbol{x})) < \infty$. Then it holds 
\begin{align*}
    \sup_{\boldsymbol{x} \in \mathcal{X}} \| \alpha f(\boldsymbol{w}_{\alpha}(T), \boldsymbol{x}) - \alpha \bar{f}(\boldsymbol{\bar{w}}_{\alpha}(T), \boldsymbol{x}) \|_2 \leq C \frac{\log(\alpha)}{\alpha}\left(M_1 + \frac{1}{2}C\cdot M_2 \cdot \log(\alpha) \right) \longrightarrow 0 \quad \text{as $\alpha \longrightarrow \infty$}.
\end{align*}
\end{manualproposition}

\begin{proof}
To start, we clarify that, unlike in Theorem \ref{finitehorizon}, the distance between $\alpha f(\boldsymbol{w}_{\alpha}(T), \boldsymbol{x})$ and $\alpha \bar{f}(\boldsymbol{\bar{w}}_{\alpha}(T), \boldsymbol{x})$ is measured in the $\ell^2$ norm for $\mathbb{R}^k$, not the Hilbert space $\mathcal{F}$ norm, since the functions $f(\boldsymbol{w}_{\alpha}(T), \boldsymbol{x})$ and $\bar{f}(\boldsymbol{\bar{w}}_{\alpha}(T), \boldsymbol{x})$ are evaluated at a particular input $\boldsymbol{x} \in \mathcal{X}$. For the same reason, $D_{\boldsymbol{w}}f(\boldsymbol{w}_0, \boldsymbol{x}) \in \mathbb{R}^{k \times d}$ is a matrix rather than a function $D_{\boldsymbol{w}}h(\boldsymbol{w}_0): \mathbb{R}^p \rightarrow \mathcal{F}$, and so $\| D_{\boldsymbol{w}}f(\boldsymbol{w}_0, \boldsymbol{x}) \|$ is taken with respect to the matrix norm $\| \cdot \|_{k, d}$.

Now that we cleared up the statement of the proposition, we appeal to the properties of the supremum to split the quantity that we wish to bound into two ancillary quantities:
\begin{align*}
    &\sup_{\boldsymbol{x} \in \mathcal{X}} \| \alpha f(\boldsymbol{w}_{\alpha}(T), x) - \alpha \bar{f}(\boldsymbol{\bar{w}}_{\alpha}(T), \boldsymbol{x}) \|_2\\
    \leq& \sup_{\boldsymbol{x} \in \mathcal{X}} \bigg( \| \alpha f(\boldsymbol{w}_{\alpha}(T), x) - \alpha \bar{f}(\boldsymbol{w}_{\alpha}(T), \boldsymbol{x}) \|_2 + \| \alpha \bar{f}(\boldsymbol{w}_{\alpha}(T), x) - \alpha \bar{f}(\boldsymbol{\bar{w}}_{\alpha}(T), \boldsymbol{x}) \|_2 \bigg) & \text{triangle inequality}\\
    \leq& \sup_{\boldsymbol{x} \in \mathcal{X}} \| \alpha f(\boldsymbol{w}_{\alpha}(T), x) - \alpha \bar{f}(\boldsymbol{w}_{\alpha}(T), \boldsymbol{x}) \|_2 + \sup_{\boldsymbol{x} \in \mathcal{X}}\| \alpha \bar{f}(\boldsymbol{w}_{\alpha}(T), x) - \alpha \bar{f}(\boldsymbol{\bar{w}}_{\alpha}(T), \boldsymbol{x}) \|_2.
\end{align*}
And so we see that it suffices to bound each term separately.

Let us start with the first term $\sup_{\boldsymbol{x} \in \mathcal{X}} \| \alpha f(\boldsymbol{w}_{\alpha}(T), x) - \alpha \bar{f}(\boldsymbol{w}_{\alpha}(T), \boldsymbol{x}) \|$. One will recall from Section \ref{prelim} that $\bar{f}(\boldsymbol{w}_{\alpha}(T), \boldsymbol{x}) = f(\boldsymbol{w}_0) + D_{\boldsymbol{w}}f(\boldsymbol{w}_0, \boldsymbol{x})(\boldsymbol{w}_{\alpha}(T) - \boldsymbol{w}_0)$ is simply equal to the first-order approximation of $f(\boldsymbol{w}_{\alpha}(T), \boldsymbol{x})$ about $\boldsymbol{w} = \boldsymbol{w}_0$. Therefore, for each fixed $\boldsymbol{x} \in \mathcal{X}$, writing $f(\boldsymbol{w}_{\alpha}(T), \boldsymbol{x}) = f(\boldsymbol{w}_0) + D_{\boldsymbol{w}}f(\boldsymbol{w}_0, \boldsymbol{x})(\boldsymbol{w}_{\alpha}(T) - \boldsymbol{w}_0) + \mathcal{R}(\boldsymbol{w}_{\alpha}(T), \boldsymbol{x})$, we have that $\| \alpha f(\boldsymbol{w}_{\alpha}(T), x) - \alpha \bar{f}(\boldsymbol{w}_{\alpha}(T), \boldsymbol{x}) \|_2 = \alpha \|\mathcal{R}(\boldsymbol{w}_{\alpha}(T), \boldsymbol{x})\|_2$, where $\mathcal{R}(\cdot, \boldsymbol{x}): \mathbb{R}^d \rightarrow \mathbb{R}^k$ is the Taylor remainder of our approximation $\bar{f}$. And so we see that if we can bound the norm of the remainder term $\mathcal{R}(\boldsymbol{w}_{\alpha}(T), \boldsymbol{x})$ for $\boldsymbol{x} \in \mathcal{X}$, then we have a bound on $\sup_{\boldsymbol{x} \in \mathcal{X}} \| \alpha f(\boldsymbol{w}_{\alpha}(T), x) - \alpha \bar{f}(\boldsymbol{w}_{\alpha}(T), \boldsymbol{x}) \|$.

In order to bound this remainder term, let us define the function $g: \mathbb{R} \rightarrow \mathbb{R}^k$ such that $g(t) = f(\boldsymbol{w}_0 + t(\boldsymbol{w}_{\alpha}(T) - \boldsymbol{w}_0), \boldsymbol{x})$. Note that since $\boldsymbol{w} \mapsto f(\boldsymbol{w}, \boldsymbol{x})$ is differentiable, by assumption, then $g(t)$ is differentiable in $t \in \mathbb{R}$. And so by the Fundamental Theorem of Calculus, we have
\begin{align*}
f(\boldsymbol{w}_{\alpha}(T), \boldsymbol{x}) - f(\boldsymbol{w}_0, \boldsymbol{x}) &= g(1) - g(0)\\
&= \int_0^1 g'(t) \ dt\\
&= \int_0^1 D_{\boldsymbol{w}}f(\boldsymbol{w}_0 + t(\boldsymbol{w}_{\alpha}(T) - \boldsymbol{w}_0), \boldsymbol{x})(\boldsymbol{w}_{\alpha}(T) - \boldsymbol{w}_0) \ dt.
\end{align*}
Just as in our proof of Theorem \ref{finitehorizon}, we remark that since $g: \mathbb{R} \rightarrow \mathbb{R}^k$, then the integral is defined component-wise. Now, by adding and subtracting the term $D_{\boldsymbol{w}}f(\boldsymbol{w}_0, \boldsymbol{x})(\boldsymbol{w}_{\alpha}(T) - \boldsymbol{w}_0)$ in the integrand, we get
\begin{align*}
    &f(\boldsymbol{w}_{\alpha}(T), \boldsymbol{x}) - f(\boldsymbol{w}_0, \boldsymbol{x})\\
    &= \int_0^1 \bigg( D_{\boldsymbol{w}}f(\boldsymbol{w}_0 + t(\boldsymbol{w}_{\alpha}(T) - \boldsymbol{w}_0), \boldsymbol{x}) - D_{\boldsymbol{w}}f(\boldsymbol{w}_0, \boldsymbol{x}) \bigg) (\boldsymbol{w}_{\alpha}(T) - \boldsymbol{w}_0) \ dt + \int_0^1 D_{\boldsymbol{w}}f(\boldsymbol{w}_0, \boldsymbol{x}) (\boldsymbol{w}_{\alpha}(T) - \boldsymbol{w}_0) \ dt\\
    &= D_{\boldsymbol{w}}f(\boldsymbol{w}_0, \boldsymbol{x}) (\boldsymbol{w}_{\alpha}(T) - \boldsymbol{w}_0) + \int_0^1 \bigg( D_{\boldsymbol{w}}f(\boldsymbol{w}_0 + t(\boldsymbol{w}_{\alpha}(T) - \boldsymbol{w}_0), \boldsymbol{x}) - D_{\boldsymbol{w}}f(\boldsymbol{w}_0, \boldsymbol{x}) \bigg) (\boldsymbol{w}_{\alpha}(T) - \boldsymbol{w}_0) \ dt.
\end{align*}
However, by subtracting the term $D_{\boldsymbol{w}}f(\boldsymbol{w}_0, \boldsymbol{x}) (\boldsymbol{w}_{\alpha}(T) - \boldsymbol{w}_0)$ over to the left-hand side of the inequality,
\begin{align*}
    f(\boldsymbol{w}_{\alpha}(T), \boldsymbol{x}) - \bar{f}(\boldsymbol{w}_{\alpha}(T), \boldsymbol{x}) \leq \int_0^1 \bigg( D_{\boldsymbol{w}}f(\boldsymbol{w}_0 + t(\boldsymbol{w}_{\alpha}(T) - \boldsymbol{w}_0), \boldsymbol{x}) - D_{\boldsymbol{w}}f(\boldsymbol{w}_0, \boldsymbol{x}) \bigg) (\boldsymbol{w}_{\alpha}(T) - \boldsymbol{w}_0) \ dt.
\end{align*}
And so to place a bound on the norm of the Taylor remainder $\mathcal{R}(\boldsymbol{w}_{\alpha}(T), \boldsymbol{x})$, we must bound the norm of the right hand side of the above inequality. Exploiting the properties of the norm, we have
\begin{align*}
    \| f(\boldsymbol{w}_{\alpha}(T), \boldsymbol{x}) - \bar{f}(\boldsymbol{w}_{\alpha}(T), \boldsymbol{x})\| &\leq \left\Vert \int_0^1 \bigg( D_{\boldsymbol{w}}f(\boldsymbol{w}_0 + t(\boldsymbol{w}_{\alpha}(T) - \boldsymbol{w}_0), \boldsymbol{x}) - D_{\boldsymbol{w}}f(\boldsymbol{w}_0, \boldsymbol{x}) \bigg) (\boldsymbol{w}_{\alpha}(T) - \boldsymbol{w}_0) \ dt \right\Vert_2\\
    &\leq \int_0^1 \left\Vert \bigg( D_{\boldsymbol{w}}f(\boldsymbol{w}_0 + t(\boldsymbol{w}_{\alpha}(T) - \boldsymbol{w}_0), \boldsymbol{x}) - D_{\boldsymbol{w}}f(\boldsymbol{w}_0, \boldsymbol{x}) \bigg)  (\boldsymbol{w}_{\alpha}(T) - \boldsymbol{w}_0) \right\Vert_2 \ dt\\
    &\leq \int_0^1 \|D_{\boldsymbol{w}}f(\boldsymbol{w}_0 + t(\boldsymbol{w}_{\alpha}(T) - \boldsymbol{w}_0), \boldsymbol{x}) - D_{\boldsymbol{w}}f(\boldsymbol{w}_0, \boldsymbol{x}) \|  \|\boldsymbol{w}_{\alpha}(T) - \boldsymbol{w}_0 \|_2 \ dt\\
    &\leq \int_0^1 \text{Lip}(\boldsymbol{w} \mapsto D_{\boldsymbol{w}}f(\boldsymbol{w}, \boldsymbol{x}))\|(\boldsymbol{w}_0 + t(\boldsymbol{w}_{\alpha}(T) - \boldsymbol{w}_0)) - \boldsymbol{w}_0 \|_2 \|\boldsymbol{w}_{\alpha}(T) - \boldsymbol{w}_0 \|_2 \ dt\\
    &= \text{Lip}(\boldsymbol{w} \mapsto D_{\boldsymbol{w}}f(\boldsymbol{w}, \boldsymbol{x})) \int_0^1 t\|\boldsymbol{w}_{\alpha}(T) - \boldsymbol{w}_0 \|_2^2 \ dt\\
    &=\frac{1}{2}\text{Lip}(\boldsymbol{w} \mapsto D_{\boldsymbol{w}}f(\boldsymbol{w}, \boldsymbol{x})) \|\boldsymbol{w}_{\alpha}(T) - \boldsymbol{w}_0 \|_2^2. 
\end{align*}
In particular, for the third inequality we invoke the property of the matrix norm $\|A\boldsymbol{v}\|_k \leq \|A\|_{k, d} \| \boldsymbol{v} \|_d$. And for the fourth inequality, we use the result from Theorem \ref{finitehorizon} that the map $\boldsymbol{w} \mapsto D_{\boldsymbol{w}}f(\boldsymbol{w}, \boldsymbol{x})$ is Lipschitz on some closed Euclidean ball containing the gradient flow path $\boldsymbol{w}_{\alpha}(t)$, $0 \leq t \leq T$. More specifically, by Theorem \ref{finitehorizon} we know that $\sup_{t \in [0, T]}\| \boldsymbol{w}_{\alpha}(t) - \boldsymbol{w}_0 \|_2 = \mathcal{O}(1/\alpha)$, and so $D_{\boldsymbol{w}}f(\boldsymbol{w}, \boldsymbol{x})$ is locally Lipschitz, by assumption, on some closed Euclidean ball containing the gradient flow path $\boldsymbol{w}_{\alpha}(t)$, $0 \leq t \leq T$; this implies $D_{\boldsymbol{w}}f(\boldsymbol{w}, \boldsymbol{x})$ is Lipschitz on the Euclidean ball. As a consequence of this bound on the norm of the Taylor remainder,
\begin{align*}
    \sup_{\boldsymbol{x} \in \mathcal{X}} \| \alpha f(\boldsymbol{w}_{\alpha}(T), x) - \alpha \bar{f}(\boldsymbol{w}_{\alpha}(T), \boldsymbol{x}) \| \leq& \sup_{\boldsymbol{x} \in \mathcal{X}} \frac{\alpha}{2}\text{Lip}(\boldsymbol{w} \mapsto D_{\boldsymbol{w}}f(\boldsymbol{w}, \boldsymbol{x})) \|\boldsymbol{w}_{\alpha}(T) - \boldsymbol{w}_0 \|_2^2\\
    \leq& \frac{\alpha}{2}\|\boldsymbol{w}_{\alpha}(T) - \boldsymbol{w}_0 \|_2^2  \sup_{\boldsymbol{x} \in \mathcal{X}}\text{Lip}(\boldsymbol{w} \mapsto D_{\boldsymbol{w}}f(\boldsymbol{w}, \boldsymbol{x}))\\
    \leq& \frac{\alpha}{2}M_2\|\boldsymbol{w}_{\alpha}(T) - \boldsymbol{w}_0 \|_2^2.
\end{align*}
Lastly, appealing to our assumption that the bounds we derived in Theorem \ref{finitehorizon} indeed hold, then we have $\|\boldsymbol{w}_{\alpha}(T) - \boldsymbol{w}_0 \|_2^2 \leq C_2^2 \log(\alpha)^2/\alpha^2$. Consequently, we attain
\begin{align*}
    \sup_{\boldsymbol{x} \in \mathcal{X}} \| \alpha f(\boldsymbol{w}_{\alpha}(T), x) - \alpha \bar{f}(\boldsymbol{w}_{\alpha}(T), \boldsymbol{x}) \| \leq \frac{M_2C_2^2 \log(\alpha)^2}{2\alpha}.
\end{align*}
Notice that for this first bound, we only used information about how far the gradient flow path $(\boldsymbol{w}_{\alpha}(t))_{t \geq 0}$ is from its initialization $\boldsymbol{w}_{\alpha}(0) = \boldsymbol{w}_0$ at time $T > 0$ and not how far the two gradient flow paths $(\boldsymbol{w}_{\alpha}(t))_{t \geq 0}$ and $(\boldsymbol{\bar{w}}_{\alpha}(t))_{t \geq 0}$ are from one another at time $T$. The second term we bound will capture the distance between these two gradient flow paths.

Specifically, we wish to derive a bound on $\sup_{\boldsymbol{x} \in \mathcal{X}}\| \alpha \bar{f}(\boldsymbol{w}_{\alpha}(T), x) - \alpha \bar{f}(\boldsymbol{\bar{w}}_{\alpha}(T), \boldsymbol{x}) \|_2$. By the definition of the linearized model $\bar{f}(\boldsymbol{w}, \boldsymbol{x})$, we have 
\begin{align*}
    &\sup_{\boldsymbol{x} \in \mathcal{X}}\| \alpha \bar{f}(\boldsymbol{w}_{\alpha}(T), x) - \alpha \bar{f}(\boldsymbol{\bar{w}}_{\alpha}(T), \boldsymbol{x}) \|_2\\
    =& \sup_{\boldsymbol{x} \in \mathcal{X}} \alpha\| (f(\boldsymbol{w}_0, \boldsymbol{x}) + D_{\boldsymbol{w}}f(\boldsymbol{w}_0, \boldsymbol{x})(\boldsymbol{w}_{\alpha}(T) - \boldsymbol{w}_0)) - (f(\boldsymbol{w}_0, \boldsymbol{x}) + D_{\boldsymbol{w}}f(\boldsymbol{w}_0, \boldsymbol{x})(\boldsymbol{\bar{w}}_{\alpha}(T) - \boldsymbol{w}_0)) \|_2\\
    =& \sup_{\boldsymbol{x} \in \mathcal{X}} \alpha \|D_{\boldsymbol{w}}f(\boldsymbol{w}_0, \boldsymbol{x})(\boldsymbol{w}_{\alpha}(T) -  \boldsymbol{\bar{w}}_{\alpha}(T)) \|_2.
\end{align*}
And so by the properties of the matrix norm $\| \cdot \|_{k, d}$, we have
\begin{align*}
    \sup_{\boldsymbol{x} \in \mathcal{X}}\| \alpha \bar{f}(\boldsymbol{w}_{\alpha}(T), x) - \alpha \bar{f}(\boldsymbol{\bar{w}}_{\alpha}(T), \boldsymbol{x}) \|_2 =& \sup_{\boldsymbol{x} \in \mathcal{X}} \alpha \|D_{\boldsymbol{w}}f(\boldsymbol{w}_0, \boldsymbol{x})(\boldsymbol{w}_{\alpha}(T) -  \boldsymbol{\bar{w}}_{\alpha}(T)) \|_2\\
    \leq& \alpha \|\boldsymbol{w}_{\alpha}(T) -  \boldsymbol{\bar{w}}_{\alpha}(T) \|_2 \sup_{\boldsymbol{x} \in \mathcal{X}} \|D_{\boldsymbol{w}}f(\boldsymbol{w}_0, \boldsymbol{x})\|\\
    \leq& \alpha M_1\|\boldsymbol{w}_{\alpha}(T) -  \boldsymbol{\bar{w}}_{\alpha}(T) \|_2.
\end{align*}
Lastly, by our bound from Theorem \ref{finitehorizon} on the distance between the gradient flow paths of $F_{\alpha}(\boldsymbol{w})$, $(\boldsymbol{w}_{\alpha}(t))_{t \geq 0}$, and $\bar{F}_{\alpha}(\boldsymbol{w})$, $(\boldsymbol{\bar{w}}_{\alpha}(t))_{t \geq 0}$, we deduce
\begin{align*}
     \sup_{\boldsymbol{x} \in \mathcal{X}}\| \alpha \bar{f}(\boldsymbol{w}_{\alpha}(T), x) - \alpha \bar{f}(\boldsymbol{\bar{w}}_{\alpha}(T), \boldsymbol{x}) \|_2 \leq \frac{M_1C_1 \log(\alpha)}{\alpha} .
\end{align*}

Altogether, we have proven 
\begin{align*}
    \sup_{\boldsymbol{x} \in \mathcal{X}} \| \alpha f(\boldsymbol{w}_{\alpha}(T), \boldsymbol{x}) - \alpha \bar{f}(\boldsymbol{\bar{w}}_{\alpha}(T), \boldsymbol{x}) \|_2 \leq \frac{M_2C_2^2 \log(\alpha)^2}{2\alpha} + \frac{M_1C_1 \log(\alpha)}{\alpha},
\end{align*}
which implies that 
\begin{align*}
    \sup_{\boldsymbol{x} \in \mathcal{X}} \| \alpha f(\boldsymbol{w}_{\alpha}(T), \boldsymbol{x}) - \alpha \bar{f}(\boldsymbol{\bar{w}}_{\alpha}(T), \boldsymbol{x}) \|_2 \longrightarrow 0 \quad \text{as} \quad \alpha \longrightarrow \infty.
\end{align*}
\end{proof}
And so we have shown that for a certain subset $\mathcal{X}$ of the input space $\mathbb{R}^d$ on which the derivative of $f$ at $\boldsymbol{w}_0$ has bounded norm and $\text{Lip}(\boldsymbol{w} \rightarrow D_{\boldsymbol{w}}f(\boldsymbol{w}, \boldsymbol{x}))$ is bounded, $\alpha f(\boldsymbol{w}, \boldsymbol{x})$ indeed generalizes like the linearized model $\alpha \bar{f}(\boldsymbol{w}, \boldsymbol{x})$ in limit $\alpha \rightarrow \infty$.

\subsection{Extending to Uniform-time Bounds}

In Section \ref{finitebounds} we delineated the conditions under which lazy training occurs and provided mathematical characterizations for lazy training that build upon our intuitive understanding from Sections \ref{introduction} and \ref{prelim}. Still, from the practictioner's perspective, the results we have presented are of little utility. Specifically, each of the bounds in Theorem \ref{finitehorizon} is dependent on a finite time horizon $T > 0$, meaning that the theoretical convergence we proved may be difficult to observe in practice for large time $t > 0$. As we pointed out, this is problematic because in order to approximate the limit of the gradient flow paths $F_{\alpha}(\boldsymbol{w})$ and $\bar{F}_{\alpha}(\boldsymbol{w})$, we must observe $\boldsymbol{w}_{\alpha}(t)$ and $\boldsymbol{\bar{w}}_{\alpha}(t)$ for large $t \geq 0$.

To partially remedy this drawback of Theorem \ref{finitehorizon}, Chizat and colleagues impose additional assumptions on the model $h$ and loss $R$ in order to achieve uniform convergence in time $t > 0$. These assumptions are summarized by Theorem \ref{uniformbound}:
\begin{manualtheorem}{2.4}\label{uniformbound}
Consider the $M$-smooth and $m$-strongly convex loss $R$ with minimizer $y^{\star}$ and condition number $\kappa := M/m$. Assume that $\sigma_{\text{min}}$, the smallest singular value of $Dh(\boldsymbol{w}_0)^T$, is positive and that the initialization satisfies $\| h(\boldsymbol{w}_0) \| \leq C_0:= \sigma_{\text{min}}^3/(32\kappa^{3/2} \| Dh(\boldsymbol{w}_0) \| \text{Lip}(Dh))$, where $\text{Lip}(Dh)$ is the Lipschitz constant of $Dh$. If $\alpha > \| y^{\star} \| / C_0$, then for $t \geq 0$, it holds
\begin{align*}
    \| \alpha h(\boldsymbol{w}_{\alpha}(t)) - y^{\star} \| \leq \sqrt{\kappa} \| \alpha h(\boldsymbol{w}_0) - y^{\star} \| \exp( -m \sigma_{\text{min}}^2 t/4).
\end{align*}
If moreover $h(\boldsymbol{w}_0) = 0$, it holds as $\alpha \rightarrow \infty$, $\sup_{t \geq 0} \left\Vert \boldsymbol{w}_{\alpha}(t) - \boldsymbol{w}_0 \right\Vert = \mathcal{O}(1/\alpha)$,
\begin{align*}
    \sup_{t \geq 0} \| \alpha h(\boldsymbol{w}_{\alpha}(t)) - \alpha \bar{h}(\boldsymbol{\bar{w}}_{\alpha}(t)) \| = \mathcal{O}(1/\alpha) \quad \text{and} \quad  \sup_{t \geq 0} \| \boldsymbol{w}_{\alpha}(t) - \boldsymbol{\bar{w}}_{\alpha}(t) \| = \mathcal{O}(\log \alpha/\alpha^2).
\end{align*}
\end{manualtheorem}
The first assumption the authors make is that the loss function $R$ is \enquote{nice} to the extent that it is both $M$-smooth and $m$-strongly convex. The even stronger assumption, though, is that $Dh(\boldsymbol{w}_0): \mathbb{R}^p \rightarrow \mathcal{F}$ is surjective, which can only be the case if the Hilbert space $\mathcal{F}$ us finite dimensional. An example of a problem which satisfies these conditions is the least-squares problem with model $h: \mathbb{R}^{2d} \rightarrow (\mathbb{R}^d)^*$: 
\begin{align*}
    h(\boldsymbol{w}) = f(\boldsymbol{w}, \boldsymbol{x}) = \langle \boldsymbol{\beta}_{\boldsymbol{w}}, \boldsymbol{x} \rangle, \quad \boldsymbol{\beta}_{\boldsymbol{w}} = \boldsymbol{w}_+^2 - \boldsymbol{w}_-^2, \quad \boldsymbol{w} = 
    \begin{bmatrix}
        \boldsymbol{w}_+ \\
        \boldsymbol{w}_-
    \end{bmatrix} 
    \in \mathbb{R}^{2d}
\end{align*}
and loss function
\begin{align*}
    R(f) = \mathbb{E}_{(\boldsymbol{x}, y) \sim \mathcal{D}}[(y - f(\boldsymbol{x}))^2] = \sum_{i=1}^N (y_i - f(\boldsymbol{x}_i))^2, \quad f \in \mathcal{F}
\end{align*}
Here, $\boldsymbol{v}^2$ denotes element-wise squaring of the vector $\boldsymbol{v} \in \mathbb{R}^{d}$, and $\mathcal{D}$ denotes the empirical distribution corresponding to training data $\{(\boldsymbol{x}_i, y_i) \}_{i=1}^N$. %with $N \geq n$%

By introducing these additional assumptions, we see that Theorem \ref{uniformbound} indeed relaxes each of the bounds from Theorem \ref{finitehorizon} to be uniform in time $t>0$. That is, Theorem \ref{uniformbound} tells us that the convergence of the gradient flow path $\boldsymbol{w}_{\alpha}(t)$ to $\boldsymbol{\bar{w}}_{\alpha}(t)$ and the corresponding model $\alpha h(\boldsymbol{w}_{\alpha}(t))$ to $\alpha \bar{h}(\boldsymbol{w}_{\alpha}(t))$ is uniform in $t > 0$ as $\alpha \rightarrow \infty$. Just as salient, Theorem \ref{uniformbound} also tells us that for sufficiently large $\alpha \geq 0$, the model $\alpha h(\boldsymbol{w}_{\alpha}(t))$ converges exponentially to a global minimizer $y^{\star}$ of the loss $R$ in the gradient flow limit $t \rightarrow \infty$. Considering that the objective function $F_{\alpha}(\boldsymbol{w})$ is not a priori convex, the fact that we achieve convergence of the gradient flow to a global minimum of the loss is a remarkable result.

Now that we have stated and expanded upon Theorem \ref{uniformbound} by Chizat and colleagues, we consider their proof of the result. Since the proof makes up the majority of the Appendix and is quite mathematically challenging, we settle for a basic overview. For more details, I encourage 

\section{Extensions of Lazy Training}\label{extensions}

\pagebreak
\bibliographystyle{siam}
\bibliography{biblio}

\end{document}
